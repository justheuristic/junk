{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43871e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n",
      "env: TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
      "env: OMP_NUM_THREADS=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/anaconda3/lib/python3.9/site-packages/torch/jit/annotations.py:386: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated bits / param 1.875 TODO ADJUST FOR STATISTICS\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3\n",
    "%env TRANSFORMERS_CACHE=/mnt/LLM/hub\n",
    "%env OMP_NUM_THREADS=16\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import time\n",
    "from tqdm.auto import trange\n",
    "import ipynbname  # pip install ipynbname\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "from src.aq import QuantizedWeight, _reconstruct_weight  # see adjacent file (aq.py)\n",
    "from src.utils import calc_avg_bits, get_mean_nbits_by_codebook  # see adjacent file (aq.py)\n",
    "from src.kmeans_1d import fit_kmeans_1d\n",
    "\n",
    "torch.set_num_threads(16)\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_loading_dir = '/extra_disk_1/vahe1994/BRRR/layer10.self_attn.q_proj.input_activation.pt'\n",
    "num_codebooks = 2\n",
    "nbits_per_codebook = 14\n",
    "out_group_size = 1\n",
    "in_group_size = 16\n",
    "batch_size = 16384\n",
    "beam_size = 1\n",
    "big_beam_size = 1\n",
    "beam_search_epochs = 100\n",
    "big_beam_search_epochs = 1000\n",
    "sparsity_regularizer = 0\n",
    "print_frequency = 10\n",
    "fit_groupwise_statistics = False\n",
    "symmetric = True  # only used if fit_groupwise_statistics; disables zero\n",
    "statistics_nbits = 16  # 16 means no compression\n",
    "\n",
    "#for each group we store num_codebooks * nbits_per_codebook - bits \n",
    "# for W matrix  we store out_features*(in_features // group_size) * num_codebooks * nbits_per_codebook - bits \n",
    "# 1 codebook store codebook_size*group_size*16 - bit \n",
    "# all codebooks store num_codebooks* codebook_size*group_size*16 -bits\n",
    "in_features, out_features  = 8192, 8192\n",
    "\n",
    "estimated_bits_per_param = calc_avg_bits(num_codebooks, out_group_size, in_group_size,\n",
    "                                         nbits_per_codebook, in_features,out_features)\n",
    "print(\"Estimated bits / param\", estimated_bits_per_param,  \"TODO ADJUST FOR STATISTICS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e6156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjustheuristic\u001b[0m (\u001b[33mrock-and-roll\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jheuristic/GPTAQ_scales/Notebooks/wandb/run-20231204_175409-q71nugwu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rock-and-roll/AddQuantization-debug/runs/q71nugwu' target=\"_blank\">aq_simple_AQ_num_codebooks=2_out_group_size=1_in_group_size=16_nbits_per_codebook=14_beam_search_epochs=100_big_beam_search_epochs=1000</a></strong> to <a href='https://wandb.ai/rock-and-roll/AddQuantization-debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rock-and-roll/AddQuantization-debug' target=\"_blank\">https://wandb.ai/rock-and-roll/AddQuantization-debug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rock-and-roll/AddQuantization-debug/runs/q71nugwu' target=\"_blank\">https://wandb.ai/rock-and-roll/AddQuantization-debug/runs/q71nugwu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No relevant files were detected in the specified directory. No code will be logged to your run.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.join(os.getcwd(), ipynbname.name() + \".ipynb\")\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    dir =os.getcwd(),\n",
    "    project=\"AddQuantization-debug\",\n",
    "    entity = \"rock-and-roll\",\n",
    "    save_code=True,\n",
    "    name = f\"{ipynbname.name()}_AQ_{num_codebooks=}_{out_group_size=}_{in_group_size=}_{nbits_per_codebook=}_{beam_search_epochs=}_{big_beam_search_epochs=}\",\n",
    "    settings=wandb.Settings(code_dir=\".\"),\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"num_codebooks\" : num_codebooks,\n",
    "    \"out_group_size\": out_group_size,\n",
    "    \"in_group_size\": in_group_size,\n",
    "    \"group_size\" : out_group_size * in_group_size,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"beam_size\" : beam_size,\n",
    "    \"big_beam_size\" : big_beam_size,\n",
    "    \"nbits_per_codebook\" : nbits_per_codebook,\n",
    "    \"Avg_bits\": estimated_bits_per_param,\n",
    "    \"beam_search_epochs\": beam_search_epochs,\n",
    "    \"big_beam_search_epochs\": big_beam_search_epochs,\n",
    "    \"sparsity_regularizer\": sparsity_regularizer,\n",
    "    \"fit_groupwise_statistics\": fit_groupwise_statistics,\n",
    "    \"symmetric\": symmetric,\n",
    "    \"statistics_nbits\": statistics_nbits,\n",
    "    }\n",
    ")\n",
    "run.log({\"Avg_bits\": estimated_bits_per_param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf7e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c294c9b8e2413ebef7958ceadcc0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-70b-hf\",\n",
    "                                                          torch_dtype='auto', low_cpu_mem_usage=True)\n",
    "\n",
    "X = torch.load(input_loading_dir,\n",
    "               map_location='cpu').float().flatten(0, -2)\n",
    "\n",
    "reference_weight = model.model.layers[10].self_attn.q_proj.weight.detach().cuda().float()\n",
    "\n",
    "XTX = torch.zeros(X.shape[-1], X.shape[-1], device=device, dtype=torch.float64)\n",
    "for i in range(0, len(X), batch_size):\n",
    "    x_batch = X[i: i + batch_size].cuda().double()\n",
    "    XTX.addmm_(x_batch.T, x_batch, alpha=1/len(X))\n",
    "    del x_batch\n",
    "XTX = XTX.float()\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897e1d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f04c04907e4e7bbcd7a5318f2ac987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "initializing with kmeans:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/GPTAQ_scales/Notebooks/../src/aq.py:435: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392022560/work/aten/src/ATen/native/cuda/Indexing.cu:1193.)\n",
      "  codebook_i, codes_i, reconstructed_weight_i = fit_kmeans(weight_residue, k=codebook_size, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "quantized_layer = QuantizedWeight(\n",
    "    reference_weight=reference_weight, num_codebooks=num_codebooks,\n",
    "    nbits_per_codebook=nbits_per_codebook,\n",
    "    out_group_size=out_group_size, in_group_size=in_group_size,\n",
    "    max_iter=100, # shorter init; not tested\n",
    "    fit_groupwise_statistics=fit_groupwise_statistics, symmetric=symmetric,\n",
    "    device=device, verbose=True\n",
    ")\n",
    "opt = torch.optim.Adam(quantized_layer.parameters(), lr=1e-5, betas=(0.9, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310b492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=0.0231684252\t time_on_epoch 0 = 1.4369528979295865\n",
      "loss=0.0215683424\t time_on_epoch 10 = 0.1284313490614295\n",
      "loss=0.0200969355\t time_on_epoch 20 = 0.12843395792879164\n",
      "loss=0.0187551878\t time_on_epoch 30 = 0.12843356898520142\n",
      "loss=0.0175383547\t time_on_epoch 40 = 0.12842697999440134\n",
      "loss=0.0164382382\t time_on_epoch 50 = 0.12844527896959335\n",
      "loss=0.0154454281\t time_on_epoch 60 = 0.1284722100244835\n",
      "loss=0.0145505908\t time_on_epoch 70 = 0.1284981200005859\n",
      "loss=0.0137449758\t time_on_epoch 80 = 0.12849316000938416\n",
      "loss=0.0130204764\t time_on_epoch 90 = 0.12843598204199225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbca788e5e61436e908578fdd662195a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(40_000):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    reconstructed_weight = _reconstruct_weight(\n",
    "        quantized_layer.codes, quantized_layer.codebooks, quantized_layer.scales, quantized_layer.zeros)\n",
    "    delta_weight = (reconstructed_weight - reference_weight).double()\n",
    "    loss = (delta_weight @ XTX.double()).flatten() @ delta_weight.flatten() / len(delta_weight)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    run.log({'loss':loss.item()}, step=epoch)\n",
    "    \n",
    "    if epoch % print_frequency == 0:\n",
    "        print(f\"loss={loss.item():.10f}\\t\",\n",
    "              f\"time_on_epoch {epoch} = {time.perf_counter() - start}\")\n",
    "    if (epoch + 1) % beam_search_epochs == 0:\n",
    "        if (epoch + 1) % big_beam_search_epochs == 0:\n",
    "            print(\"BIG beam search\")\n",
    "        quantized_layer.requantize_(\n",
    "            XTX, reference_weight,\n",
    "            beam_size=beam_size if (epoch + 1) % 1000 != 0 else big_beam_size,\n",
    "            sparsity_regularizer=sparsity_regularizer,  # tip: use const_hparam * quantized_layer.codes.numel()\n",
    "            verbose=True)\n",
    "        if sparsity_regularizer != 0:\n",
    "            sparsity_rate = ((quantized_layer.codes == 0).sum() / quantized_layer.codes.numel()).item()\n",
    "            print(f\"Sparsity rate {sparsity_rate:.5f}\")\n",
    "            run.log({'sparsity rate': sparsity_rate}, stepii=epoch)\n",
    "            if (epoch + 1) % big_beam_search_epochs == 0:\n",
    "                mean_code_nbits = sum(get_mean_nbits_by_codebook(quantized_layer.codes)) / num_codebooks\n",
    "                print(f\"mean_code_nbits {mean_code_nbits:.5f}\")\n",
    "                run.log({'Mean codebook ldngth nbits': mean_code_nbits}, step=epoch)\n",
    "                if in_group_size > 1 and out_group_size > 1:\n",
    "                    curr_avg_bits  = calc_avg_bits(num_codebooks, 1, mean_code_nbits,\n",
    "                                         nbits_per_codebook, in_features,out_features)\n",
    "                    run.log({\"Avg_bits\": curr_avg_bits}, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b4534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
